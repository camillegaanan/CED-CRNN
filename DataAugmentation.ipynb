{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#DATA AUGMENTATION\r\n",
    "\r\n",
    "# Importing necessary functions\r\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n",
    "import os\r\n",
    "\r\n",
    "def augment_images(image_path, medicine):\r\n",
    "\t\t# Loading a sample image\r\n",
    "\t\timg = load_img(image_path)\r\n",
    "\t\t# Converting the input sample image to an array\r\n",
    "\t\tx = img_to_array(img)\r\n",
    "\t\t# Reshaping the input image\r\n",
    "\t\tx = x.reshape((1, ) + x.shape)\r\n",
    "\r\n",
    "\t\t# Generating and saving 5 augmented samples\r\n",
    "\t\t# using the above defined parameters.\r\n",
    "\t\ti = 0\r\n",
    "\t\tfor batch in datagen.flow(x, batch_size = 16,\r\n",
    "\t\t\t\t\t\t\t\t#save_to_dir ='C:/Users/Camille/Downloads/NEW PATH', #Camille\r\n",
    "\t\t\t\t\t\t\t\tsave_to_dir ='D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS', #Ash\r\n",
    "\t\t\t\t\t\t\t\tsave_prefix =medicine, save_format ='jpg'):\r\n",
    "\t\t\ti += 1\r\n",
    "\t\t\tif i > 5:\r\n",
    "\t\t\t\tbreak\r\n",
    "\r\n",
    "# Initialising the ImageDataGenerator class.\r\n",
    "# We will pass in the augmentation parameters in the constructor.\r\n",
    "datagen = ImageDataGenerator(\r\n",
    "\t\trotation_range = 5,\r\n",
    "\t\tshear_range = 1,\r\n",
    "\t\tzoom_range = 0.2,\r\n",
    "\t\trescale=1)\r\n",
    "\r\n",
    "#input_path = 'C:/Users/Camille/Downloads/CROPPED LABELED RESIZED PRES PICS' #Camille\r\n",
    "input_path = 'D:\\Program Files\\CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "for filename in os.listdir(input_path):\r\n",
    "\tif filename.endswith(\".jpg\"):\r\n",
    "\t\tif filename.find(\"Azathioprine\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Azathioprine\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Ceftriaxone\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Ceftriaxone\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Chlorpromazine\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Chlorpromazine\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Dobutamine\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Dobutamine\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Hydroxyzine\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Hydroxyzine\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Lorazepam\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Lorazepam\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Metronidazole\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Metronidazole\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Prednisolone\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Prednisolone\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Quinine\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Quinine\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Risperidone\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Risperidone\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telif filename.find(\"Rituximab\") != -1:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Rituximab\"\r\n",
    "\t\t\taugment_images(image_path, medicine)\r\n",
    "\t\telse:\r\n",
    "\t\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\t\tmedicine = \"Tramadol\"\r\n",
    "\t\t\taugment_images(image_path, medicine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# # Creating Train / Val / Test folders (One time use)\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "import random\r\n",
    "#root_dir = 'C:/Users/Camille/Downloads/splitted data/' #Camille\r\n",
    "root_dir = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/' #Ash\r\n",
    "\r\n",
    "\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.3\r\n",
    "\r\n",
    "os.makedirs(root_dir +'train/')\r\n",
    "os.makedirs(root_dir +'test/')\r\n",
    "\r\n",
    "# Creating partitions of the data after shuffeling\r\n",
    "#src = 'C:/Users/Camille/Downloads/NEW PATH' # Folder to copy images from --Camille\r\n",
    "src = 'D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "\r\n",
    "\r\n",
    "allFileNames = os.listdir(src)\r\n",
    "np.random.shuffle(allFileNames)\r\n",
    "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\r\n",
    "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \r\n",
    "                                                           int(len(allFileNames)* (1 - test_ratio))])\r\n",
    "\r\n",
    "\r\n",
    "train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\r\n",
    "test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\r\n",
    "\r\n",
    "print('Total images: ', len(allFileNames))\r\n",
    "print('Training: ', len(train_FileNames))\r\n",
    "#print('Validation: ', len(val_FileNames))\r\n",
    "print('Testing: ', len(test_FileNames))\r\n",
    "\r\n",
    "# Copy-pasting images\r\n",
    "for name in train_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'train/' )\r\n",
    "\r\n",
    "#for name in val_FileNames:\r\n",
    " #   shutil.copy(name, root_dir +'val/' + cls)\r\n",
    "\r\n",
    "for name in test_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'test/' )"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train/'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20428/2255849323.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtest_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'test/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train/'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#BINARIZATION\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from skimage.filters import threshold_otsu\r\n",
    "\r\n",
    "def binary(image):\r\n",
    "    (thresh, im_bw) = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\r\n",
    "    thresh = 127\r\n",
    "    im_bw = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)[1]\r\n",
    "    return im_bw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#NOISE REMOVAL\r\n",
    "import bm3d\r\n",
    "def noise(bw):\r\n",
    "    den = bm3d.bm3d(bw, sigma_psd=30/255, stage_arg=bm3d.BM3DStages.HARD_THRESHOLDING)\r\n",
    "    return den"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#CED\r\n",
    "from scipy import signal\r\n",
    "\r\n",
    "def ced(image):\r\n",
    "    m1 = np.array([[5, 5, 5],[-3,0,-3],[-3,-3,-3]])\r\n",
    "    m8 = np.array([[-3, 5,5],[-3,0,5],[-3,-3,-3]])\r\n",
    "    m7 = np.array([[-3,-3,5],[-3,0,5],[-3,-3,5]])\r\n",
    "    m6 = np.array([[-3,-3,-3],[-3,0,5],[-3,5,5]])\r\n",
    "    m5 = np.array([[-3, -3, -3],[-3,0,-3],[5,5,5]])\r\n",
    "    m4 = np.array([[-3, -3, -3],[5,0,-3],[5,5,-3]])\r\n",
    "    m3 = np.array([[5, -3, -3],[5,0,-3],[5,-3,-3]])\r\n",
    "    m2 = np.array([[5, 5, -3],[5,0,-3],[-3,-3,-3]])\r\n",
    "    list_m = [m1,m2,m3,m4,m5,m6,m7,m8]\r\n",
    "\r\n",
    "    list_e = []\r\n",
    "    count = 1\r\n",
    "    \r\n",
    "    for m in list_m:\r\n",
    "        imgk = signal.convolve2d(image, m,boundary='symm')\r\n",
    "        list_e.append(np.abs(imgk))\r\n",
    "        out = imgk.astype(np.uint8)\r\n",
    "        count += 1\r\n",
    "    #Seeking maximum\r\n",
    "    count\r\n",
    "    e = list_e[0]\r\n",
    "    for i in range(len(list_e)):\r\n",
    "        e = e*(e>=list_e[i]) + list_e[i]*(e<list_e[i])\r\n",
    "        \r\n",
    "    e[e>255] = 255\r\n",
    "    e=e.astype(np.uint8)\r\n",
    "    return e"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from skimage.morphology import skeletonize\r\n",
    "from skimage import data\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from skimage.util import invert\r\n",
    "\r\n",
    "def skeleton(image):\r\n",
    "    return cv2.ximgproc.thinning(image)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#PREPROCESSING IMAGES\r\n",
    "import time\r\n",
    "import matplotlib.image as imgsave\r\n",
    "# import pandas as pd\r\n",
    "#filename = 'C:/Users/Camille/Downloads/splitted data/train/'\r\n",
    "#filename2 = 'C:/Users/Camille/Downloads/splitted data/test/'\r\n",
    "filename = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train/' #Ash\r\n",
    "filename2 = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/test/' #Ash\r\n",
    "def preprocess(train_dir):\r\n",
    "    start_time = time.time()\r\n",
    "    list_images = []\r\n",
    "    for filename in os.listdir(train_dir):\r\n",
    "        image = cv2.imread(os.path.join(train_dir, filename),0)\r\n",
    "        img2 = binary(image)\r\n",
    "        # img3 = noise(img2)\r\n",
    "        imgg = cv2.blur(img2,(3,3))\r\n",
    "        img4 = ced(imgg)\r\n",
    "        img5 = skeleton(img4)\r\n",
    "        list_images.append(img5)\r\n",
    "    print(\"--- %s seconds --- skeleton image\" % (time.time() - start_time))\r\n",
    "    list_images = np.array(list_images, np.int64)\r\n",
    "    return list_images\r\n",
    "\r\n",
    "list_train_images = preprocess(filename)\r\n",
    "list_test_images = preprocess(filename2)\r\n",
    "print(list_train_images)\r\n",
    "print(list_test_images)\r\n",
    "\r\n",
    "# filepath='C:/Users/Camille/Downloads/pagod na ko/'\r\n",
    "\r\n",
    "# image = cv2.imread('C:/Users/Camille/Downloads/splitted data/train/Azathioprine_0_2228.jpg',0)\r\n",
    "# img = binary(image)\r\n",
    "# img2 = noise(img)\r\n",
    "\r\n",
    "# img3 = cv2.blur(img2, ksize=(3,3))\r\n",
    "# img4 = ced(img3)\r\n",
    "# img5 = skeleton(img4)\r\n",
    "\r\n",
    "# cv2.imwrite(os.path.join(filepath,\"1grey.jpg\"), image)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"2binary.jpg\"), img)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"3noise.jpg\"), img2)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"4blur.jpg\"), img3)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"5ced.jpg\"), img4) \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"6skeleton.jpg\"), img5) \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 8.166945219039917 seconds --- skeleton image\n",
      "--- 4.353776216506958 seconds --- skeleton image\n",
      "[[[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0 255]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]]\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import tensorflow as tf\r\n",
    "\r\n",
    "# from tensorflow.keras import datasets, layers, models\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n",
    "\r\n",
    "# # print(train_images)\r\n",
    "# image = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# image2 = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# list1 = []\r\n",
    "# list1.append(image)\r\n",
    "# list1.append(image2)\r\n",
    "# list2 = np.array(list1, np.int32)\r\n",
    "# print(list2)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "bb314e96b9025c5714465989964256ab98bfab43e576c1152b046cbb0a30c64a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}