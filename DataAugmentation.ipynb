{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#DATA AUGMENTATION\r\n",
    "\r\n",
    "# Importing necessary functions\r\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\r\n",
    "import os\r\n",
    "\r\n",
    "def augment_images(image_path, filename):\r\n",
    "\t\t# Loading a sample image\r\n",
    "\t\timg = load_img(image_path)\r\n",
    "\t\t# Converting the input sample image to an array\r\n",
    "\t\tx = img_to_array(img)\r\n",
    "\t\t# Reshaping the input image\r\n",
    "\t\tx = x.reshape((1, ) + x.shape)\r\n",
    "\r\n",
    "\t\t# Generating and saving 5 augmented samples\r\n",
    "\t\t# using the above defined parameters.\r\n",
    "\t\ti = 0\r\n",
    "\t\tfor batch in datagen.flow(x, batch_size = 16,\r\n",
    "\t\t\t\t\t\t\t\tsave_to_dir ='C:/Users/Camille/Downloads/NEW PATH', #Camille\r\n",
    "\t\t\t\t\t\t\t\t# save_to_dir ='D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS', #Ash\r\n",
    "\t\t\t\t\t\t\t\tsave_prefix =filename+'_'+str(i), save_format ='jpg'):\r\n",
    "\t\t\ti += 1\r\n",
    "\t\t\tif i > 5:\r\n",
    "\t\t\t\tbreak\r\n",
    "\r\n",
    "# Initialising the ImageDataGenerator class.\r\n",
    "# We will pass in the augmentation parameters in the constructor.\r\n",
    "datagen = ImageDataGenerator(\r\n",
    "\t\trotation_range = 5,\r\n",
    "\t\tshear_range = 1,\r\n",
    "\t\tzoom_range = 0.2,\r\n",
    "\t\trescale=1)\r\n",
    "\r\n",
    "input_path = 'C:/Users/Camille/Downloads/CROPPED LABELED RESIZED PRES PICS' #Camille\r\n",
    "# input_path = 'D:\\Program Files\\CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "for filename in os.listdir(input_path):\r\n",
    "\tif filename.endswith(\".jpg\"):\r\n",
    "\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\taugment_images(image_path, filename[:-4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# # Creating Train / Val / Test folders (One time use)\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "root_dir = 'C:/Users/Camille/Downloads/splitted data/' #Camille\r\n",
    "# root_dir = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/' #Ash\r\n",
    "\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.3\r\n",
    "\r\n",
    "os.makedirs(root_dir +'train/')\r\n",
    "os.makedirs(root_dir +'test/')\r\n",
    "\r\n",
    "# Creating partitions of the data after shuffeling\r\n",
    "src = 'C:/Users/Camille/Downloads/NEW PATH' # Folder to copy images from --Camille\r\n",
    "# src = 'D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "\r\n",
    "\r\n",
    "allFileNames = os.listdir(src)\r\n",
    "np.random.shuffle(allFileNames)\r\n",
    "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\r\n",
    "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \r\n",
    "                                                           int(len(allFileNames)* (1 - test_ratio))])\r\n",
    "\r\n",
    "\r\n",
    "train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\r\n",
    "test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\r\n",
    "\r\n",
    "print('Total images: ', len(allFileNames))\r\n",
    "print('Training: ', len(train_FileNames))\r\n",
    "#print('Validation: ', len(val_FileNames))\r\n",
    "print('Testing: ', len(test_FileNames))\r\n",
    "\r\n",
    "# Copy-pasting images\r\n",
    "for name in train_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'train/' )\r\n",
    "\r\n",
    "#for name in val_FileNames:\r\n",
    " #   shutil.copy(name, root_dir +'val/' + cls)\r\n",
    "\r\n",
    "for name in test_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'test/' )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total images:  2585\n",
      "Training:  1809\n",
      "Testing:  776\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "#BINARIZATION\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def binary(image):\r\n",
    "    (thresh, im_bw) = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\r\n",
    "    thresh = 127\r\n",
    "    im_bw = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)[1]\r\n",
    "    return im_bw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "#NOISE REMOVAL\r\n",
    "import cv2\r\n",
    "def noise(bw):\r\n",
    "    # den = bm3d.bm3d(bw, sigma_psd=30/255, stage_arg=bm3d.BM3DStages.HARD_THRESHOLDING)\r\n",
    "    den = cv2.fastNlMeansDenoising(bw, None, 10, 7, 15)\r\n",
    "    return den"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "#CED\r\n",
    "from scipy import signal\r\n",
    "def ced(image):\r\n",
    "    m1 = np.array([[5, 5, 5],[-3,0,-3],[-3,-3,-3]])\r\n",
    "    m8 = np.array([[-3, 5,5],[-3,0,5],[-3,-3,-3]])\r\n",
    "    m7 = np.array([[-3,-3,5],[-3,0,5],[-3,-3,5]])\r\n",
    "    m6 = np.array([[-3,-3,-3],[-3,0,5],[-3,5,5]])\r\n",
    "    m5 = np.array([[-3, -3, -3],[-3,0,-3],[5,5,5]])\r\n",
    "    m4 = np.array([[-3, -3, -3],[5,0,-3],[5,5,-3]])\r\n",
    "    m3 = np.array([[5, -3, -3],[5,0,-3],[5,-3,-3]])\r\n",
    "    m2 = np.array([[5, 5, -3],[5,0,-3],[-3,-3,-3]])\r\n",
    "    list_m = [m1,m2,m3,m4,m5,m6,m7,m8]\r\n",
    "\r\n",
    "    list_e = []\r\n",
    "    count = 1\r\n",
    "    for m in list_m:\r\n",
    "        imgk = signal.convolve2d(image, m,boundary='symm')\r\n",
    "        list_e.append(np.abs(imgk))\r\n",
    "        out = imgk.astype(np.uint8)\r\n",
    "        count += 1\r\n",
    "    #Seeking maximum\r\n",
    "    count\r\n",
    "    e = list_e[0]\r\n",
    "    for i in range(len(list_e)):\r\n",
    "        e = e*(e>=list_e[i]) + list_e[i]*(e<list_e[i])\r\n",
    "        \r\n",
    "    e[e>255] = 255\r\n",
    "    e=e.astype(np.uint8)\r\n",
    "    return e"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "import cv2\r\n",
    "def skeleton(image):\r\n",
    "    return cv2.ximgproc.thinning(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "#PREPROCESSING IMAGES ver 1, 6-8min\r\n",
    "import time\r\n",
    "import cv2\r\n",
    "# import pandas as pd\r\n",
    "filename = 'C:/Users/Camille/Downloads/splitted data/train/'\r\n",
    "filename2 = 'C:/Users/Camille/Downloads/splitted data/test/'\r\n",
    "# filename = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train/' #Ash\r\n",
    "# filename2 = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/test/' #Ash\r\n",
    "def preprocess(train_dir):\r\n",
    "    start_time = time.time()\r\n",
    "    list_images = []\r\n",
    "    for filename in os.listdir(train_dir):\r\n",
    "        image = cv2.imread(os.path.join(train_dir, filename),0)\r\n",
    "        img2 = binary(image)\r\n",
    "        img3 = noise(img2)\r\n",
    "        imgg = cv2.blur(img3,(3,3))\r\n",
    "        img4 = ced(imgg)\r\n",
    "        img5 = skeleton(img4)\r\n",
    "        list_images.append(img5)\r\n",
    "    print(\"--- %s seconds --- image\" % (time.time() - start_time))\r\n",
    "    list_images = np.array(list_images, np.int32)\r\n",
    "    return list_images\r\n",
    "\r\n",
    "list_train_images = preprocess(filename)\r\n",
    "list_test_images = preprocess(filename2)\r\n",
    "print(list_train_images)\r\n",
    "print(list_test_images)\r\n",
    "\r\n",
    "# filepath='C:/Users/Camille/Downloads/pagod na ko/'\r\n",
    "\r\n",
    "# image = cv2.imread('C:/Users/Camille/Downloads/splitted data/train/Hydroxyzine_0_8889.jpg',0)\r\n",
    "\r\n",
    "# img = binary(image)\r\n",
    "# img2 = noise(img)\r\n",
    "\r\n",
    "# img3 = cv2.blur(img2, ksize=(3,3))\r\n",
    "\r\n",
    "# img4 = ced(img3)\r\n",
    "# img5 = skeleton(img4)\r\n",
    "\r\n",
    "# cv2.imwrite(os.path.join(filepath,\"1grey.jpg\"), image)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"2binary.jpg\"), img)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"3noise.jpg\"), img2)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"4blur.jpg\"), img3)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"5ced.jpg\"), img4) \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"6skeleton.jpg\"), img5) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 345.5207951068878 seconds --- image\n",
      "--- 144.41565203666687 seconds --- image\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# import tensorflow as tf\r\n",
    "\r\n",
    "# from tensorflow.keras import datasets, layers, models\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n",
    "\r\n",
    "# # print(train_images)\r\n",
    "# # image = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# # image2 = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# # list1 = []\r\n",
    "# # list1.append(image)\r\n",
    "# # list1.append(image2)\r\n",
    "# # list2 = np.array(list1, np.int32)\r\n",
    "# print(train_images)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "#CNN ASH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#CNN CAM\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "#CNN TRIX"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "#CNN CARL"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "6c0057e970611b3440242629919342879023e6b08fd2b04f89b50c3ebc2a3cfa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}