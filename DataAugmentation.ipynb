{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#DATA AUGMENTATION\r\n",
    "\r\n",
    "# Importing necessary functions\r\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\r\n",
    "import os\r\n",
    "\r\n",
    "def augment_images(image_path, filename):\r\n",
    "\t\t# Loading a sample image\r\n",
    "\t\timg = load_img(image_path)\r\n",
    "\t\t# Converting the input sample image to an array\r\n",
    "\t\tx = img_to_array(img)\r\n",
    "\t\t# Reshaping the input image\r\n",
    "\t\tx = x.reshape((1, ) + x.shape)\r\n",
    "\r\n",
    "\t\t# Generating and saving 5 augmented samples\r\n",
    "\t\t# using the above defined parameters.\r\n",
    "\t\ti = 0\r\n",
    "\t\tfor batch in datagen.flow(x, batch_size = 16,\r\n",
    "\t\t\t\t\t\t\t\tsave_to_dir ='C:/Users/Camille/Downloads/NEW PATH 2', #Camille\r\n",
    "\t\t\t\t\t\t\t\t# save_to_dir ='D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS', #Ash\r\n",
    "\t\t\t\t\t\t\t\tsave_prefix =filename+'_'+str(i), save_format ='jpg'):\r\n",
    "\t\t\ti += 1\r\n",
    "\t\t\tif i > 5:\r\n",
    "\t\t\t\tbreak\r\n",
    "\r\n",
    "# Initialising the ImageDataGenerator class.\r\n",
    "# We will pass in the augmentation parameters in the constructor.\r\n",
    "datagen = ImageDataGenerator(\r\n",
    "\t\trotation_range = 5,\r\n",
    "\t\tshear_range = 1,\r\n",
    "\t\tzoom_range = 0.2,\r\n",
    "\t\trescale=1)\r\n",
    "\r\n",
    "input_path = 'C:/Users/Camille/Downloads/CROPPED LABELED RESIZED PRES PICS 2' #Camille\r\n",
    "# input_path = 'D:\\Program Files\\CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "for filename in os.listdir(input_path):\r\n",
    "\tif filename.endswith(\".jpg\"):\r\n",
    "\t\timage_path = os.path.join(input_path, filename)\r\n",
    "\t\taugment_images(image_path, filename[:-4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# # Creating Train / Val / Test folders (One time use)\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "import random\r\n",
    "root_dir = 'C:/Users/Camille/Downloads/splitted data 2/' #Camille\r\n",
    "# root_dir = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/' #Ash\r\n",
    "\r\n",
    "\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.3\r\n",
    "\r\n",
    "os.makedirs(root_dir +'train/')\r\n",
    "os.makedirs(root_dir +'test/')\r\n",
    "\r\n",
    "# Creating partitions of the data after shuffeling\r\n",
    "src = 'C:/Users/Camille/Downloads/NEW PATH 2' # Folder to copy images from --Camille\r\n",
    "# src = 'D:/Program Files/NEW CROPPED LABELED RESIZED PRES PICS' #Ash\r\n",
    "\r\n",
    "\r\n",
    "allFileNames = os.listdir(src)\r\n",
    "np.random.shuffle(allFileNames)\r\n",
    "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\r\n",
    "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \r\n",
    "                                                           int(len(allFileNames)* (1 - test_ratio))])\r\n",
    "\r\n",
    "\r\n",
    "train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\r\n",
    "test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\r\n",
    "\r\n",
    "print('Total images: ', len(allFileNames))\r\n",
    "print('Training: ', len(train_FileNames))\r\n",
    "#print('Validation: ', len(val_FileNames))\r\n",
    "print('Testing: ', len(test_FileNames))\r\n",
    "\r\n",
    "# Copy-pasting images\r\n",
    "for name in train_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'train/' )\r\n",
    "\r\n",
    "#for name in val_FileNames:\r\n",
    " #   shutil.copy(name, root_dir +'val/' + cls)\r\n",
    "\r\n",
    "for name in test_FileNames:\r\n",
    "    shutil.copy(name, root_dir +'test/' )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total images:  288\n",
      "Training:  201\n",
      "Testing:  87\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#BINARIZATION\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from skimage.filters import threshold_otsu\r\n",
    "\r\n",
    "def binary(image):\r\n",
    "    (thresh, im_bw) = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\r\n",
    "    thresh = 127\r\n",
    "    im_bw = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)[1]\r\n",
    "    return im_bw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#NOISE REMOVAL\r\n",
    "import bm3d\r\n",
    "def noise(bw):\r\n",
    "    # den = bm3d.bm3d(bw, sigma_psd=30/255, stage_arg=bm3d.BM3DStages.HARD_THRESHOLDING)\r\n",
    "    den = cv2.fastNlMeansDenoising(bw, None, 10, 7, 15)\r\n",
    "    return den"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#CED\r\n",
    "from scipy import signal\r\n",
    "\r\n",
    "def ced(image):\r\n",
    "    m1 = np.array([[5, 5, 5],[-3,0,-3],[-3,-3,-3]])\r\n",
    "    m8 = np.array([[-3, 5,5],[-3,0,5],[-3,-3,-3]])\r\n",
    "    m7 = np.array([[-3,-3,5],[-3,0,5],[-3,-3,5]])\r\n",
    "    m6 = np.array([[-3,-3,-3],[-3,0,5],[-3,5,5]])\r\n",
    "    m5 = np.array([[-3, -3, -3],[-3,0,-3],[5,5,5]])\r\n",
    "    m4 = np.array([[-3, -3, -3],[5,0,-3],[5,5,-3]])\r\n",
    "    m3 = np.array([[5, -3, -3],[5,0,-3],[5,-3,-3]])\r\n",
    "    m2 = np.array([[5, 5, -3],[5,0,-3],[-3,-3,-3]])\r\n",
    "    list_m = [m1,m2,m3,m4,m5,m6,m7,m8]\r\n",
    "\r\n",
    "    list_e = []\r\n",
    "    count = 1\r\n",
    "    \r\n",
    "    for m in list_m:\r\n",
    "        imgk = signal.convolve2d(image, m,boundary='symm')\r\n",
    "        list_e.append(np.abs(imgk))\r\n",
    "        out = imgk.astype(np.uint8)\r\n",
    "        count += 1\r\n",
    "    #Seeking maximum\r\n",
    "    count\r\n",
    "    e = list_e[0]\r\n",
    "    for i in range(len(list_e)):\r\n",
    "        e = e*(e>=list_e[i]) + list_e[i]*(e<list_e[i])\r\n",
    "        \r\n",
    "    e[e>255] = 255\r\n",
    "    e=e.astype(np.uint8)\r\n",
    "    return e"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from skimage.morphology import skeletonize\r\n",
    "from skimage import data\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from skimage.util import invert\r\n",
    "\r\n",
    "def skeleton(image):\r\n",
    "    return cv2.ximgproc.thinning(image)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#PREPROCESSING IMAGES\r\n",
    "import time\r\n",
    "import matplotlib.image as imgsave\r\n",
    "# import pandas as pd\r\n",
    "filename = 'C:/Users/Camille/Downloads/splitted data 2/train/'\r\n",
    "filename2 = 'C:/Users/Camille/Downloads/splitted data 2/test/'\r\n",
    "# filename = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train/' #Ash\r\n",
    "# filename2 = 'D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/test/' #Ash\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(train_dir):\r\n",
    "    start_time = time.time()\r\n",
    "    list_images = []\r\n",
    "    for filename in os.listdir(train_dir):\r\n",
    "        image = cv2.imread(os.path.join(train_dir, filename),0)\r\n",
    "        img2 = binary(image)\r\n",
    "        img3 = noise(img2)\r\n",
    "        imgg = cv2.blur(img3,(3,3))\r\n",
    "        img4 = ced(imgg)\r\n",
    "        img5 = skeleton(img4)\r\n",
    "        # img5 = tf.convert_to_tensor(img5)\r\n",
    "        list_images.append(img5)\r\n",
    "    print(\"--- %s seconds --- skeleton image\" % (time.time() - start_time))\r\n",
    "    list_images = np.array(list_images, np.int64)\r\n",
    "    return list_images\r\n",
    "\r\n",
    "list_train_images = preprocess(filename)\r\n",
    "list_test_images = preprocess(filename2)\r\n",
    "print(list_train_images)\r\n",
    "print(list_test_images)\r\n",
    "\r\n",
    "# filepath='C:/Users/Camille/Downloads/pagod na ko/'\r\n",
    "\r\n",
    "# image = cv2.imread('C:/Users/Camille/Downloads/splitted data/train/Hydroxyzine_0_8889.jpg',0)\r\n",
    "\r\n",
    "# img = binary(image)\r\n",
    "# img2 = noise(img)\r\n",
    "\r\n",
    "# img3 = cv2.blur(img2, ksize=(3,3))\r\n",
    "\r\n",
    "# img4 = ced(img3)\r\n",
    "# img5 = skeleton(img4)\r\n",
    "\r\n",
    "# cv2.imwrite(os.path.join(filepath,\"1grey.jpg\"), image)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"2binary.jpg\"), img)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"3noise.jpg\"), img2)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"4blur.jpg\"), img3)  \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"5ced.jpg\"), img4) \r\n",
    "# cv2.imwrite(os.path.join(filepath,\"6skeleton.jpg\"), img5) \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 59.336859941482544 seconds --- skeleton image\n",
      "--- 24.08628249168396 seconds --- skeleton image\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import tensorflow as tf\r\n",
    "\r\n",
    "# from tensorflow.keras import datasets, layers, models\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n",
    "\r\n",
    "# # print(train_images)\r\n",
    "# image = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# image2 = cv2.imread(os.path.join(filepath,\"6skeleton.jpg\"))\r\n",
    "# list1 = []\r\n",
    "# list1.append(image)\r\n",
    "# list1.append(image2)\r\n",
    "# list2 = np.array(list1, np.int32)\r\n",
    "# print(list2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#CNN ASH\r\n",
    "import os\r\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
    "\r\n",
    "\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import string\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
    "import tensorflow.keras.backend as K\r\n",
    "\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, LSTM\r\n",
    "# from tensorflow.compat.v1.keras.layers import CuDNNLSTM\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.optimizers import *\r\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
    "from tqdm import tqdm\r\n",
    "from collections import Counter\r\n",
    "from PIL import Image\r\n",
    "from itertools import groupby\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "# image_paths = []\r\n",
    "image_texts = []\r\n",
    "from pathlib import Path\r\n",
    "# data_folder = \"D:/Program Files/SPLIT NEW CROPPED LABELED RESIZED PRES PICS/train\" #Ash\r\n",
    "data_folder = 'C:/Users/Camille/Downloads/splitted data 2/train'\r\n",
    "data_dir = Path(\"C:/Users/Camille/Downloads/splitted data 2/train\")\r\n",
    "image_paths = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\r\n",
    "print(len(image_paths))\r\n",
    "for filename in os.listdir(data_dir):\r\n",
    "    # image_paths.append(data_folder + \"/\" + filename)\r\n",
    "    # image_texts.append(filename.split(\"/\")[0])\r\n",
    "    if filename.find(\"Azathioprine\") != -1:\r\n",
    "        image_texts.append(\"Azathioprine: 3-5 mg/kg Per os OD\")\r\n",
    "        # image_texts.append(\"Azathioprine 3 5 mg kg Per os OD\")\r\n",
    "    elif filename.find(\"Ceftriaxone\") != -1:\r\n",
    "        image_texts.append(\"Ceftriaxone: 2 g IV q24h\")\r\n",
    "        # image_texts.append(\"Ceftriaxone 2 g IV q24h\")\r\n",
    "    elif filename.find(\"Chlorpromazine\") != -1:\r\n",
    "        image_texts.append(\"Chlorpromazine: 10-25 mg Per os three times a day\")\r\n",
    "        # image_texts.append(\"Chlorpromazine 10 25 mg Per os three times a day\")\r\n",
    "    elif filename.find(\"Dobutamine\") != -1:\r\n",
    "        image_texts.append(\"Dobutamine: 2.5-15 mcg/kg/min\")\r\n",
    "        # image_texts.append(\"Dobutamine 2 5 15 mcg kg min\")\r\n",
    "    elif filename.find(\"Hydroxyzine\") != -1:\r\n",
    "        image_texts.append(\"Hydroxyzine: 50-100 mg by IJ qds\")\r\n",
    "        # image_texts.append(\"Hydroxyzine 50 100 mg by IJ qds\")\r\n",
    "    elif filename.find(\"Lorazepam\") != -1:\r\n",
    "        image_texts.append(\"Lorazepam: 1 mg Per os 2 times a day\")\r\n",
    "        # image_texts.append(\"Lorazepam 1 mg Per os 2 times a day\")\r\n",
    "    elif filename.find(\"Metronidazole\") != -1:\r\n",
    "        image_texts.append(\"Metronidazole: 7.5 mg/kg Per os q6hr\")\r\n",
    "        # image_texts.append(\"Metronidazole 7 5 mg kg Per os q6hr\")\r\n",
    "    elif filename.find(\"Prednisolone\") != -1:\r\n",
    "        image_texts.append(\"Prednisolone: 5-60 mg per day qds\")\r\n",
    "        # image_texts.append(\"Prednisolone 5 60 mg per day qds\")\r\n",
    "    elif filename.find(\"Quinine\") != -1:\r\n",
    "        image_texts.append(\"Quinine: 648 mg Per os every 8 hours for 7 days\")\r\n",
    "        # image_texts.append(\"Quinine 648 mg Per os every 8 hours for 7 days\")\r\n",
    "    elif filename.find(\"Risperidone\") != -1:\r\n",
    "        image_texts.append(\"Risperidone: 2 mg orally i/d\")\r\n",
    "        # image_texts.append(\"Risperidone 2 mg orally i d\")\r\n",
    "    elif filename.find(\"Rituximab\") != -1:\r\n",
    "        image_texts.append(\"Rituximab: 375 mg/m2 IV once weekly\")\r\n",
    "        # image_texts.append(\"Rituximab 375 mg m2 IV once weekly\")\r\n",
    "    else:\r\n",
    "        image_texts.append(\"Tramadol: 50-100 mg as needed every 4 to 6 hours\")\r\n",
    "        # image_texts.append(\"Tramadol 50100 mg as needed every 4 to 6 hours\")\r\n",
    "print(type(image_paths))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "201\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "# char_list = set(['0','1','2','3','4','5','6','7','8','A','C','D','H','I','J','L','M','O','P','Q','R','T','V','a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' '])\r\n",
    "char_list = ['-', '.', '/','0','1','2','3','4','5','6','7','8',':','A','C','D','H','I','J','L','M','O','P','Q','R','T','V','a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' ']\r\n",
    "# char_list = set(['0','1','2','3','4','5','6','7','8','A','C','D','H','I','J','L','M','O','P','Q','R','T','V','a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' '])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "max_label_len = 49"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "def split_data(images, labels, train_size=0.9, shuffle=True):\r\n",
    "    # 1. Get the total size of the dataset\r\n",
    "    size = len(images)\r\n",
    "    # 2. Make an indices array and shuffle it, if required\r\n",
    "    indices = np.arange(size)\r\n",
    "    if shuffle:\r\n",
    "        np.random.shuffle(indices)\r\n",
    "    # 3. Get the size of training samples\r\n",
    "    train_samples = int(size * train_size)\r\n",
    "    # 4. Split data into training and validation sets\r\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\r\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\r\n",
    "    return x_train, x_valid, y_train, y_valid\r\n",
    "\r\n",
    "# Splitting data into training and validation sets\r\n",
    "train_image_paths, val_image_paths, train_image_texts, val_image_texts = split_data(np.array(image_paths), np.array(image_texts))\r\n",
    "print(len(train_image_paths))\r\n",
    "print(len(val_image_paths))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "180\n",
      "21\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "source": [
    "def encode_to_labels(txt):\r\n",
    "    # encoding each output word into digits\r\n",
    "    dig_lst = []\r\n",
    "    \r\n",
    "    for index, char in enumerate(txt):\r\n",
    "        try:\r\n",
    "            dig_lst.append(char_list.index(char))\r\n",
    "        except:\r\n",
    "            print(char)\r\n",
    "\r\n",
    "    return pad_sequences([dig_lst], maxlen=max_label_len, padding='post', value=len(char_list))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "# padded_image_texts = list(map(encode_to_labels, image_texts))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "# train_image_paths = image_paths[ : int(len(image_paths) * 0.90)]\r\n",
    "\r\n",
    "# train_image_texts = padded_image_texts[ : int(len(image_texts) * 0.90)]\r\n",
    "# print(len(train_image_texts))\r\n",
    "# val_image_paths = image_paths[int(len(image_paths) * 0.90) : ]\r\n",
    "# val_image_texts = padded_image_texts[int(len(image_texts) * 0.90) : ]\r\n",
    "# print(len(val_image_texts))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "# Mapping characters to integers\r\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\r\n",
    "    vocabulary=list(char_list), num_oov_indices=0, mask_token=None\r\n",
    ")\r\n",
    "def process_single_sample(img_path, label):\r\n",
    "\r\n",
    "    # 1. Read image\r\n",
    "    img = tf.io.read_file(img_path)\r\n",
    "\r\n",
    "    # 2. Decode and convert to grayscale\r\n",
    "    img = tf.io.decode_png(img, channels=1)\r\n",
    "    # 3. Convert to float32 in [0, 1] range\r\n",
    "    img = tf.image.convert_image_dtype(img, tf.float64) #initially 32\r\n",
    "\r\n",
    "    # 4. Resize to the desired size\r\n",
    "    img = tf.image.resize(img, [64, 256])\r\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\r\n",
    "    # 6. Map the characters in label to numbers\r\n",
    "    #\r\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\r\n",
    "\r\n",
    "    \r\n",
    "    return {\"image\": img, \"label\": label}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "source": [
    "batch_size = 1\r\n",
    "# print(type(train_image_paths))\r\n",
    "# print(type(train_image_texts))\r\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_image_texts))\r\n",
    "# print(train_dataset)\r\n",
    "train_dataset = (\r\n",
    "    train_dataset.map(\r\n",
    "        process_single_sample, num_parallel_calls=tf.data.AUTOTUNE\r\n",
    "    )\r\n",
    "    .batch(batch_size)\r\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    ")\r\n",
    "\r\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_image_texts))\r\n",
    "validation_dataset = (\r\n",
    "    validation_dataset.map(\r\n",
    "        process_single_sample, num_parallel_calls=tf.data.AUTOTUNE\r\n",
    "    )\r\n",
    "    .batch(batch_size)\r\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "class CTCLayer(layers.Layer):\r\n",
    "\r\n",
    "    def __init__(self, name=None):\r\n",
    "        \r\n",
    "        super().__init__(name=name)        \r\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\r\n",
    "\r\n",
    "    def call(self, y_true, y_pred):\r\n",
    "        # Compute the training-time loss value and add it\r\n",
    "        # to the layer using `self.add_loss()`.\r\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\r\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\r\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\r\n",
    "\r\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\r\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\r\n",
    "\r\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\r\n",
    "        self.add_loss(loss)\r\n",
    "\r\n",
    "        # At test time, just return the computed predictions\r\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "def ctc_decoder(predictions):\r\n",
    "    '''\r\n",
    "    input: given batch of predictions from text rec model\r\n",
    "    output: return lists of raw extracted text\r\n",
    "\r\n",
    "    '''\r\n",
    "    text_list = []\r\n",
    "    \r\n",
    "    pred_indcies = np.argmax(predictions, axis=2)\r\n",
    "    \r\n",
    "    for i in range(pred_indcies.shape[0]):\r\n",
    "        ans = \"\"\r\n",
    "        \r\n",
    "        ## merge repeats\r\n",
    "        merged_list = [k for k,_ in groupby(pred_indcies[i])]\r\n",
    "        \r\n",
    "        ## remove blanks\r\n",
    "        for p in merged_list:\r\n",
    "            if p != len(char_list):\r\n",
    "                ans += char_list[int(p)]\r\n",
    "        \r\n",
    "        text_list.append(ans)\r\n",
    "        \r\n",
    "    return text_list\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "# figures_list = []\r\n",
    "\r\n",
    "# class PlotPredictions(tf.keras.callbacks.Callback):\r\n",
    "\r\n",
    "#     def __init__(self, frequency=1):\r\n",
    "#         self.frequency = frequency\r\n",
    "#         super(PlotPredictions, self).__init__()\r\n",
    "\r\n",
    "#         batch = validation_dataset.take(1)\r\n",
    "#         self.batch_images = list(batch.as_numpy_iterator())[0][\"image\"]\r\n",
    "#         self.batch_labels = list(batch.as_numpy_iterator())[0][\"label\"]\r\n",
    "\r\n",
    "#     def plot_predictions(self, epoch):\r\n",
    "\r\n",
    "#         prediction_model = keras.models.Model(\r\n",
    "#             self.model.get_layer(name=\"image\").input, \r\n",
    "#             self.model.get_layer(name=\"dense\").output\r\n",
    "#         )\r\n",
    "        \r\n",
    "#         preds = prediction_model.predict(self.batch_images)\r\n",
    "#         pred_texts = ctc_decoder(preds)\r\n",
    "\r\n",
    "#         orig_texts = []\r\n",
    "\r\n",
    "#         for label in self.batch_labels:\r\n",
    "#             orig_texts.append(\"\".join([char_list[int(char_ind)] for char_ind in label if not(char_ind == len(char_list))]))\r\n",
    "\r\n",
    "#         fig , ax = plt.subplots(4, 4, figsize=(15, 5))\r\n",
    "#         fig.suptitle('Epoch: '+str(epoch), weight='bold', size=14)\r\n",
    "\r\n",
    "#         for i in range(16):\r\n",
    "\r\n",
    "#             img = (self.batch_images[i, :, :, 0] * 255).astype(np.uint8)\r\n",
    "#             title = f\"Prediction: {pred_texts[i]}\"\r\n",
    "#             ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\r\n",
    "#             ax[i // 4, i % 4].set_title(title)\r\n",
    "#             ax[i // 4, i % 4].axis(\"off\")\r\n",
    "        \r\n",
    "#         plt.show()\r\n",
    "#         #plt.savefig(\"predictions_epoch_\"+ str(epoch)+'.png', bbox_inches = 'tight', pad_inches = 0)\r\n",
    "        \r\n",
    "#         figures_list.append(fig)\r\n",
    "\r\n",
    "#     def on_epoch_end(self, epoch, logs=None):\r\n",
    "#         if epoch % self.frequency == 0:\r\n",
    "#             self.plot_predictions(epoch) \r\n",
    "#             figures_list = []\r\n",
    "\r\n",
    "# class PlotPredictions(tf.keras.callbacks.Callback):\r\n",
    "\r\n",
    "#     def __init__(self, frequency=1):\r\n",
    "#         self.frequency = frequency\r\n",
    "#         super(PlotPredictions, self).__init__()\r\n",
    "\r\n",
    "#         batch = validation_dataset.take(1)\r\n",
    "#         self.batch_images = list(batch.as_numpy_iterator())[0][\"image\"]\r\n",
    "#         self.batch_labels = list(batch.as_numpy_iterator())[0][\"label\"]\r\n",
    "\r\n",
    "#     def plot_predictions(self, epoch):\r\n",
    "\r\n",
    "#         prediction_model = keras.models.Model(\r\n",
    "#             self.model.get_layer(name=\"image\").input, \r\n",
    "#             self.model.get_layer(name=\"dense\").output\r\n",
    "#         )\r\n",
    "        \r\n",
    "#         preds = prediction_model.predict(self.batch_images)\r\n",
    "#         pred_texts = ctc_decoder(preds)\r\n",
    "\r\n",
    "#         orig_texts = []\r\n",
    "\r\n",
    "#         for label in self.batch_labels:\r\n",
    "#             orig_texts.append(\"\".join([char_list[int(char_ind)] for char_ind in label if not(char_ind == len(char_list))]))\r\n",
    "\r\n",
    "#         fig , ax = plt.subplots(4, 4, figsize=(15, 5))\r\n",
    "#         fig.suptitle('Epoch: '+str(epoch), weight='bold', size=14)\r\n",
    "\r\n",
    "#         for i in range(16):\r\n",
    "\r\n",
    "#             img = (self.batch_images[i, :, :, 0] * 255).astype(np.uint8)\r\n",
    "#             title = f\"Prediction: {pred_texts[i]}\"\r\n",
    "#             ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\r\n",
    "#             ax[i // 4, i % 4].set_title(title)\r\n",
    "#             ax[i // 4, i % 4].axis(\"off\")\r\n",
    "        \r\n",
    "#         plt.show()\r\n",
    "#         #plt.savefig(\"predictions_epoch_\"+ str(epoch)+'.png', bbox_inches = 'tight', pad_inches = 0)\r\n",
    "        \r\n",
    "#         figures_list.append(fig)\r\n",
    "\r\n",
    "#     def on_epoch_end(self, epoch, logs=None):\r\n",
    "#         if epoch % self.frequency == 0:\r\n",
    "#             self.plot_predictions(epoch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "# def train(epochs):\r\n",
    "    \r\n",
    "#     # input with shape of height=32 and width=128 \r\n",
    "#     inputs = Input(shape=(32, 128, 1), name=\"image\", dtype=\"float32\")\r\n",
    "\r\n",
    "#     labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\r\n",
    "\r\n",
    "#     conv_1 = Conv2D(64, (3,3), activation = \"selu\", padding='same')(inputs)\r\n",
    "#     pool_1 = MaxPool2D(pool_size=(2, 2))(conv_1)\r\n",
    "    \r\n",
    "#     conv_2 = Conv2D(64, (3,3), activation = \"selu\", padding='same')(pool_1)\r\n",
    "#     pool_2 = MaxPool2D(pool_size=(2, 2))(conv_2)\r\n",
    "\r\n",
    "#     conv_3 = Conv2D(128, (3,3), activation = \"selu\", padding='same')(pool_2)\r\n",
    "#     conv_4 = Conv2D(128, (3,3), activation = \"selu\", padding='same')(conv_3)\r\n",
    "\r\n",
    "#     pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\r\n",
    "    \r\n",
    "#     conv_5 = Conv2D(256, (3,3), activation = \"selu\", padding='same')(pool_4)\r\n",
    "    \r\n",
    "#     # Batch normalization layer\r\n",
    "#     batch_norm_5 = BatchNormalization()(conv_5)\r\n",
    "    \r\n",
    "#     conv_6 = Conv2D(256, (3,3), activation = \"selu\", padding='same')(batch_norm_5)\r\n",
    "#     batch_norm_6 = BatchNormalization()(conv_6)\r\n",
    "#     pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\r\n",
    "    \r\n",
    "#     conv_7 = Conv2D(64, (2,2), activation = \"selu\")(pool_6)\r\n",
    "    \r\n",
    "#     squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\r\n",
    "    \r\n",
    "#     # bidirectional LSTM layers with units=128\r\n",
    "#     blstm_1 = Bidirectional(LSTM(128, return_sequences=True))(squeezed)\r\n",
    "#     blstm_2 = Bidirectional(LSTM(128, return_sequences=True))(blstm_1)\r\n",
    "\r\n",
    "#     softmax_output = Dense(len(char_list) + 1, activation = 'softmax', name=\"dense\")(blstm_2)\r\n",
    "\r\n",
    "#     output = CTCLayer(name=\"ctc_loss\")(labels, softmax_output)\r\n",
    "\r\n",
    "\r\n",
    "#     optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\r\n",
    "\r\n",
    "#     #model to be used at training time\r\n",
    "#     model = Model(inputs=[inputs, labels], outputs=output)\r\n",
    "#     model.compile(optimizer = optimizer)\r\n",
    "\r\n",
    "#     print(model.summary())\r\n",
    "#     file_path = \"C_LSTM_best.hdf5\"\r\n",
    "    \r\n",
    "#     checkpoint = ModelCheckpoint(filepath=file_path, \r\n",
    "#                                 monitor='val_loss', \r\n",
    "#                                 verbose=1, \r\n",
    "#                                 save_best_only=True, \r\n",
    "#                                 mode='min')\r\n",
    "\r\n",
    "#     callbacks_list = [checkpoint, \r\n",
    "#                       PlotPredictions(frequency=1),\r\n",
    "#                       EarlyStopping(patience=3, verbose=1)]\r\n",
    "\r\n",
    "#     history = model.fit(train_dataset, \r\n",
    "#                         epochs = epochs,\r\n",
    "#                         validation_data=validation_dataset,\r\n",
    "#                         verbose = 1,\r\n",
    "#                         callbacks = callbacks_list,\r\n",
    "#                         shuffle=True)\r\n",
    "    \r\n",
    "#     return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "# model = train(epochs=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "source": [
    "\r\n",
    "import keras\r\n",
    "from keras import backend as K\r\n",
    "from keras.models import Sequential, Model\r\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, Reshape, Flatten, Dense\r\n",
    "from keras.layers import Bidirectional, LSTM, Lambda\r\n",
    "from keras.layers.normalization import BatchNormalization\r\n",
    "from keras.optimizers import SGD\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "import math\r\n",
    "char_list = set(['-', '.', '/',':','0','1','2','3','4','5','6','7','8',':','A','C','D','H','I','J','L','M','O','P','Q','R','T','V','a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' '])\r\n",
    "# char_list = set(['0','1','2','3','4','5','6','7','8','A','C','D','H','I','J','L','M','O','P','Q','R','T','V','a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' '])\r\n",
    "\r\n",
    "\r\n",
    "window_height = 256  #windown height\r\n",
    "window_width = 64   #window width\r\n",
    "window_shift = window_width - 2 #window shift\r\n",
    "\r\n",
    "#CNN related configurations\r\n",
    "MPoolLayers_ALL = 3\t#Nbr of all maxpool layers\r\n",
    "MPoolLayers_H = 2\t#Nbr of maxpool in horizontal dimension\r\n",
    "LastFilters = 256\t#Nbr of feature maps at the last conv layer\r\n",
    "\r\n",
    "#LSTM related configurations\r\n",
    "NUnits = 256    #Number of units in forward/backward LSTM\r\n",
    "NLayers = 2     #Number of layers in BLSTM\r\n",
    "\r\n",
    "#%%\r\n",
    "FV = int(window_height / math.pow(2, MPoolLayers_ALL))\r\n",
    "# print(FV)\r\n",
    "NFeatures = FV * LastFilters\r\n",
    "\r\n",
    "#%%\r\n",
    "input_data = Input(shape=(window_height, window_width, 1), name=\"image\")\r\n",
    "# labels = layers.Input(name=\"label\", shape=(None,))\r\n",
    "labels = layers.Input(name=\"label\", shape=(None,))\r\n",
    "convolution1 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\")(input_data)\r\n",
    "convolution1 = BatchNormalization(axis = -1)(convolution1)\r\n",
    "\r\n",
    "convolution2 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\")(convolution1)\r\n",
    "convolution2 = BatchNormalization(axis = -1)(convolution2)\r\n",
    "\r\n",
    "pooling1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(convolution2)\r\n",
    "\r\n",
    "convolution3 = Conv2D(filters=128, kernel_size=(3,3),activation='relu', padding=\"same\")(pooling1)\r\n",
    "convolution3 = BatchNormalization(axis = -1)(convolution3)\r\n",
    "\r\n",
    "convolution4 = Conv2D(filters=128, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution3)\r\n",
    "convolution4 = BatchNormalization(axis = -1)(convolution4)\r\n",
    "\r\n",
    "pooling2 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(convolution4)\r\n",
    "\r\n",
    "convolution5 = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding=\"same\")(pooling2)\r\n",
    "convolution5 = BatchNormalization(axis = -1)(convolution5)\r\n",
    "\r\n",
    "# convolution6 = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution5)\r\n",
    "# convolution6 = BatchNormalization(axis = -1)(convolution6)\r\n",
    "\r\n",
    "# convolution7 = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution6)\r\n",
    "# convolution7 = BatchNormalization(axis = -1)(convolution7)\r\n",
    "\r\n",
    "# pooling3 = MaxPooling2D(pool_size=(2,1), strides=(2,1),)(convolution7)\r\n",
    "\r\n",
    "# convolution8 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(pooling3)\r\n",
    "# convolution8 = BatchNormalization(axis = -1)(convolution8)\r\n",
    "\r\n",
    "# convolution9 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution8)\r\n",
    "# convolution9 = BatchNormalization(axis = -1)(convolution9)\r\n",
    "\r\n",
    "# convolution10 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution9)\r\n",
    "# convolution10= BatchNormalization(axis = -1)(convolution10)\r\n",
    "\r\n",
    "# pooling4 = MaxPooling2D(pool_size=(2,1), strides=(2,1))(convolution10)\r\n",
    "\r\n",
    "# convolution11 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(pooling4)\r\n",
    "# convolution11= BatchNormalization(axis = -1)(convolution11)\r\n",
    "\r\n",
    "# convolution12 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution11)\r\n",
    "# convolution12= BatchNormalization(axis = -1)(convolution12)\r\n",
    "\r\n",
    "# convolution13 = Conv2D(filters=512, kernel_size=(3,3),activation='relu', padding=\"same\")(convolution12)\r\n",
    "# convolution13= BatchNormalization(axis = -1)(convolution13)\r\n",
    "\r\n",
    "pooling5 = MaxPooling2D(pool_size=(2,1), strides=(2,1))(convolution5)\r\n",
    "# print(pooling5.shape)\r\n",
    "\r\n",
    "# new_shape = (None, (window_width // 32), (window_height // 32) * 512)\r\n",
    "# print(new_shape)\r\n",
    "# convolution_full = Reshape(target_shape=new_shape)(pooling5)\r\n",
    "convolution_full = Reshape(target_shape=(LastFilters * FV, 16))(pooling5)\r\n",
    "# x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(convolution_full)\r\n",
    "# x = layers.Dropout(0.2)(x)\r\n",
    "# flatten = Flatten()(pooling5)\r\n",
    "\r\n",
    "# convolution_full =Dense(512)(flatten)\r\n",
    "   \r\n",
    "    # bidirectional LSTM layers with units=128\r\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True))(convolution_full)\r\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True))(blstm_1)\r\n",
    "# blstm_3 = \r\n",
    "\r\n",
    "softmax_output = Dense(len(char_list) + 1, activation = 'softmax', name=\"dense\")(blstm_2)\r\n",
    "\r\n",
    "output = CTCLayer(name=\"ctc_loss\")(labels, softmax_output)\r\n",
    "\r\n",
    "\r\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\r\n",
    "\r\n",
    "    #model to be used at training time\r\n",
    "Model(inputs=[input_data, labels], outputs=output).summary()\r\n",
    "model = Model(inputs=[input_data, labels], outputs=output)\r\n",
    "model.compile(optimizer = optimizer)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 256, 64, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 256, 64, 64)  640         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 256, 64, 64)  256         conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 256, 64, 64)  36928       batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 256, 64, 64)  256         conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_150 (MaxPooling2D (None, 128, 32, 64)  0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 128, 32, 128) 73856       max_pooling2d_150[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 128, 32, 128) 512         conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 128, 32, 128) 147584      batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 128, 32, 128) 512         conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_151 (MaxPooling2D (None, 64, 16, 128)  0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 64, 16, 256)  295168      max_pooling2d_151[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 64, 16, 256)  1024        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_152 (MaxPooling2D (None, 32, 16, 256)  0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_41 (Reshape)            (None, 8192, 16)     0           max_pooling2d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_52 (Bidirectional (None, 8192, 256)    148480      reshape_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_53 (Bidirectional (None, 8192, 256)    394240      bidirectional_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8192, 54)     13878       bidirectional_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (CTCLayer)             (None, 8192, 54)     0           label[0][0]                      \n",
      "                                                                 dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,113,334\n",
      "Trainable params: 1,112,054\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "source": [
    "# model.fit(train_dataset, \r\n",
    "#                         epochs = 1,\r\n",
    "#                         validation_data=validation_dataset,\r\n",
    "#                         verbose = 1,\r\n",
    "#                         shuffle=True)\r\n",
    "\r\n",
    "epochs = 5\r\n",
    "early_stopping_patience = 10\r\n",
    "# Add early stopping\r\n",
    "early_stopping = keras.callbacks.EarlyStopping(\r\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\r\n",
    ")\r\n",
    "\r\n",
    "# Train the model\r\n",
    "model.fit(\r\n",
    "    train_dataset,\r\n",
    "    validation_data=validation_dataset,\r\n",
    "    epochs=epochs,\r\n",
    "    callbacks=[early_stopping],\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6556/3799421420.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "num_to_char = layers.experimental.preprocessing.StringLookup(\r\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\r\n",
    ")\r\n",
    "\r\n",
    "# Get the prediction model by extracting layers till the output layer\r\n",
    "prediction_model = keras.models.Model(\r\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense\").output\r\n",
    ")\r\n",
    "prediction_model.summary()\r\n",
    "\r\n",
    "# A utility function to decode the output of the network\r\n",
    "def decode_batch_predictions(pred):\r\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n",
    "    # Use greedy search. For complex tasks, you can use beam search\r\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\r\n",
    "        :, :49\r\n",
    "    ]\r\n",
    "    # Iterate over the results and get back the text\r\n",
    "    output_text = []\r\n",
    "    for res in results:\r\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\r\n",
    "        output_text.append(res)\r\n",
    "    return output_text\r\n",
    "\r\n",
    "\r\n",
    "#  Let's check results on some validation samples\r\n",
    "for batch in validation_dataset.take(1):\r\n",
    "    batch_images = batch[\"image\"]\r\n",
    "    batch_labels = batch[\"label\"]\r\n",
    "\r\n",
    "    preds = prediction_model.predict(batch_images)\r\n",
    "    pred_texts = decode_batch_predictions(preds)\r\n",
    "\r\n",
    "    orig_texts = []\r\n",
    "    for label in batch_labels:\r\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\r\n",
    "        orig_texts.append(label)\r\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 5))\r\n",
    "    for i in range(len(pred_texts)):\r\n",
    "        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\r\n",
    "        img = img.T\r\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\r\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\r\n",
    "        ax[i // 4, i % 4].set_title(title)\r\n",
    "        ax[i // 4, i % 4].axis(\"off\")\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_263 (Conv2D)          (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_263 (Bat (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_264 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_264 (Bat (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_105 (MaxPoolin (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_265 (Conv2D)          (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_265 (Bat (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_266 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_266 (Bat (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_106 (MaxPoolin (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_267 (Conv2D)          (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_267 (Bat (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_268 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_268 (Bat (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_269 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_269 (Bat (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 8, 16, 256)        0         \n",
      "_________________________________________________________________\n",
      "reshape_26 (Reshape)         (None, 2048, 16)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_46 (Bidirectio (None, 2048, 512)         559104    \n",
      "_________________________________________________________________\n",
      "bidirectional_47 (Bidirectio (None, 2048, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048, 54)          27702     \n",
      "=================================================================\n",
      "Total params: 3,900,662\n",
      "Trainable params: 3,898,358\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#CNN TRIX"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#CNN CARL"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "6c0057e970611b3440242629919342879023e6b08fd2b04f89b50c3ebc2a3cfa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}